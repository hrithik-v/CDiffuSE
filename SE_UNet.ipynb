{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "654344b0-40bd-4aa3-b989-c203f19944e4",
    "_uuid": "5bfc6689-6eb4-490a-99f3-a3a75a99f212",
    "collapsed": false,
    "id": "k1UY9qhtp5fd",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## First time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a0c075e9-0183-482c-843b-8be8d9d5f751",
    "_uuid": "f887bd59-668b-481e-ad35-05931b649e0e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-11T08:30:33.230936Z",
     "iopub.status.busy": "2025-04-11T08:30:33.230647Z",
     "iopub.status.idle": "2025-04-11T08:30:33.234888Z",
     "shell.execute_reply": "2025-04-11T08:30:33.234088Z",
     "shell.execute_reply.started": "2025-04-11T08:30:33.230914Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def dprint(expr):\n",
    "#     val = eval(expr, globals(), locals())\n",
    "#     print(f\"{expr}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "e0c9b562-8dd4-410b-aac6-4be2a123c43b",
    "_uuid": "447ad6cf-52e1-40cb-bd92-cdb439316054",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-08T16:55:15.406513Z",
     "iopub.status.busy": "2025-04-08T16:55:15.406206Z",
     "iopub.status.idle": "2025-04-08T16:55:15.425000Z",
     "shell.execute_reply": "2025-04-08T16:55:15.423882Z",
     "shell.execute_reply.started": "2025-04-08T16:55:15.406485Z"
    },
    "id": "NCDo-fff_8Ya",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install librosa torchaudio sox\n",
    "# !pip install torch torchaudio tqdm einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "2586630a-2982-4dcb-8993-3f5d00e37440",
    "_uuid": "6f97d21e-4a58-47e4-91dc-0f8985a261d3",
    "execution": {
     "iopub.execute_input": "2025-04-08T16:55:15.426276Z",
     "iopub.status.busy": "2025-04-08T16:55:15.425979Z",
     "iopub.status.idle": "2025-04-08T16:55:15.452517Z",
     "shell.execute_reply": "2025-04-08T16:55:15.451495Z",
     "shell.execute_reply.started": "2025-04-08T16:55:15.426249Z"
    },
    "id": "fBziprGXtKqO",
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !wget -O clean_trainset_28spk_wav.zip \"https://datashare.ed.ac.uk/bitstream/handle/10283/2791/clean_trainset_28spk_wav.zip\"\n",
    "# !wget -O noisy_trainset_28spk_wav.zip \"https://datashare.ed.ac.uk/bitstream/handle/10283/2791/noisy_trainset_28spk_wav.zip\"\n",
    "# !wget -O clean_testset_wav.zip \"https://datashare.ed.ac.uk/bitstream/handle/10283/2791/clean_testset_wav.zip\"\n",
    "# !wget -O noisy_testset_wav.zip \"https://datashare.ed.ac.uk/bitstream/handle/10283/2791/noisy_testset_wav.zip\"\n",
    "\n",
    "# !unzip -q clean_trainset_28spk_wav.zip -d ./VoiceBank/Clean_Train\n",
    "# !unzip -q noisy_trainset_28spk_wav.zip -d ./VoiceBank/Noisy_Train\n",
    "# !unzip -q clean_testset_wav.zip -d ./VoiceBank/Clean_Test\n",
    "# !unzip -q noisy_testset_wav.zip -d ./VoiceBank/Noisy_Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "670252fb-754a-4f9f-a652-7180046c87b6",
    "_uuid": "01787455-d273-424d-9e3b-1c758244e1b1",
    "collapsed": false,
    "id": "F1_8MVRjRnnU",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Second Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "ff916d40-ff2c-42ca-a4fc-11bf35d94090",
    "_uuid": "2ee2fc73-bb0c-4ffe-ba83-08d82c4d400f",
    "execution": {
     "iopub.execute_input": "2025-04-08T16:55:15.453712Z",
     "iopub.status.busy": "2025-04-08T16:55:15.453455Z",
     "iopub.status.idle": "2025-04-08T16:55:15.470735Z",
     "shell.execute_reply": "2025-04-08T16:55:15.469699Z",
     "shell.execute_reply.started": "2025-04-08T16:55:15.453691Z"
    },
    "executionInfo": {
     "elapsed": 1368225,
     "status": "ok",
     "timestamp": 1743502324450,
     "user": {
      "displayName": "2K22-CO-217 HRITHIK VERMA",
      "userId": "03642715821829266027"
     },
     "user_tz": -330
    },
    "id": "HydW7JLfRtEh",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "52dc5461-f7a0-4559-b0c7-0a3119e26653",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torchaudio\n",
    "# import torchaudio.transforms as transforms\n",
    "# import torch\n",
    "\n",
    "# # Define paths\n",
    "# base_dir = \"./drive/MyDrive/Datasets/\"\n",
    "# clean_train_path = os.path.join(base_dir, \"VoiceBank\", \"Clean_Train\",\"clean_trainset_28spk_wav\")\n",
    "# noisy_train_path = os.path.join(base_dir, \"VoiceBank\", \"Noisy_Train\", \"noisy_trainset_28spk_wav\")\n",
    "# output_clean = os.path.join(base_dir, \"VoiceBank_processed\", \"Clean_Train\")\n",
    "# output_noisy = os.path.join(base_dir, \"VoiceBank_processed\", \"Noisy_Train\")\n",
    "\n",
    "\n",
    "# os.makedirs(output_clean, exist_ok=True)\n",
    "# os.makedirs(output_noisy, exist_ok=True)\n",
    "\n",
    "# # Convert audio to Mel Spectrogram\n",
    "# mel_transform = transforms.MelSpectrogram(sample_rate=16000, n_mels=80)\n",
    "\n",
    "# def process_audio(file, input_dir, output_dir):\n",
    "#     file_path = os.path.join(input_dir, file)\n",
    "#     waveform, sr = torchaudio.load(file_path)\n",
    "\n",
    "#     # Convert to Mel Spectrogram\n",
    "#     mel_spec = mel_transform(waveform)\n",
    "\n",
    "#     # Save as torch tensor\n",
    "#     torch.save(mel_spec, os.path.join(output_dir, file.replace('.wav', '.pt')))\n",
    "\n",
    "# # Process dataset\n",
    "# for file in os.listdir(clean_train_path):\n",
    "#     if file.endswith(\".wav\"):\n",
    "#         process_audio(file, clean_train_path, output_clean)\n",
    "#         process_audio(file, noisy_train_path, output_noisy)\n",
    "\n",
    "# print(\"✅ Dataset preprocessed and saved as Mel spectrograms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "204adea9-9543-4707-b257-7978d9eaa073",
    "_uuid": "c149a83c-4d5f-41e0-bfc5-38c775934553",
    "collapsed": false,
    "id": "g-ODRy3IqAAt",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "f166beaa-b74c-497a-a12b-441b7ed11ef7",
    "_uuid": "99353f0b-ab89-49d8-aa85-b89b8d288d91",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-16T09:44:48.079345Z",
     "iopub.status.busy": "2025-04-16T09:44:48.078954Z",
     "iopub.status.idle": "2025-04-16T09:44:48.083794Z",
     "shell.execute_reply": "2025-04-16T09:44:48.082922Z",
     "shell.execute_reply.started": "2025-04-16T09:44:48.079317Z"
    },
    "id": "EZRU8M0cqZqg",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "base_dir = \"/kaggle/input/voicebank\"\n",
    "clean_train_path = os.path.join(base_dir, \"clean_trainset_28spk_wav\",\"clean_trainset_28spk_wav\")\n",
    "noisy_train_path = os.path.join(base_dir, \"noisy_trainset_28spk_wav\", \"noisy_trainset_28spk_wav\")\n",
    "\n",
    "output_clean = os.path.join(base_dir, \"VoiceBank_processed\", \"Clean_Train\")\n",
    "output_noisy = os.path.join(base_dir, \"VoiceBank_processed\", \"Noisy_Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cf94f09e-f519-49d7-a200-8751adc806ff",
    "_uuid": "854579e7-76a0-42ba-8d74-f4a7fe3896a8",
    "collapsed": false,
    "id": "7rbi0BfTMByU",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "Noise Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "0548dab2-0538-41c1-9069-90ed2da8492e",
    "_uuid": "6ac2cd64-b67e-4a1b-8d0d-29ec07ed810c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-16T09:44:48.643889Z",
     "iopub.status.busy": "2025-04-16T09:44:48.643565Z",
     "iopub.status.idle": "2025-04-16T09:44:48.652541Z",
     "shell.execute_reply": "2025-04-16T09:44:48.651809Z",
     "shell.execute_reply.started": "2025-04-16T09:44:48.643855Z"
    },
    "id": "XN4jchwQMDDD",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NoiseScheduler:\n",
    "    def __init__(self, timesteps=1000, s=0.008):\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        # Create a cosine schedule for alpha_bar\n",
    "        # We compute timesteps + 1 values to derive betas for the discrete intervals.\n",
    "        steps = timesteps + 1  \n",
    "        t_lin = torch.linspace(0, timesteps, steps) / timesteps  # normalized [0,1]\n",
    "        \n",
    "        # Cosine schedule as introduced in improved diffusion:\n",
    "        # f(t) = cos((t + s) / (1+s) * (pi/2))^2; normalized so f(0)=1.\n",
    "        alpha_bar = torch.cos((t_lin + s) / (1 + s) * (math.pi / 2)) ** 2\n",
    "        alpha_bar = alpha_bar / alpha_bar[0]  # ensure alpha_bar[0] is 1\n",
    "        \n",
    "        # Now, derive discrete betas from the continuous alpha_bar schedule.\n",
    "        betas = []\n",
    "        for t in range(timesteps):\n",
    "            # Clip beta to be in a reasonable range to avoid numerical issues.\n",
    "            beta = min(1 - alpha_bar[t+1] / alpha_bar[t], 0.999)\n",
    "            betas.append(beta)\n",
    "        self.beta = torch.tensor(betas)\n",
    "        \n",
    "        # Compute other parameters based on beta\n",
    "        self.alpha = 1 - self.beta\n",
    "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
    "        self.snr = self.alpha_bar / (1 - self.alpha_bar)\n",
    "    \n",
    "    def _move_to_device(self, tensor, device):\n",
    "        \"\"\"Helper to move tensors to device while preserving gradient info\"\"\"\n",
    "        return tensor.to(device, non_blocking=True, copy=False)\n",
    "    \n",
    "    def add_noise(self, x, t, noise=None):\n",
    "        \"\"\"\n",
    "        Adds noise to input x at timestep t, handling device compatibility.\n",
    "        Ensures all tensors are on the same device as input x.\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        alpha_bar = self._move_to_device(self.alpha_bar, device)\n",
    "    \n",
    "        # Reshape for broadcasting - assuming x is in NCHW or similar format.\n",
    "        sqrt_alpha_bar_t = alpha_bar[t] ** 0.5\n",
    "        sqrt_alpha_bar_t = sqrt_alpha_bar_t.view(-1, 1, 1, 1)\n",
    "    \n",
    "        sqrt_one_minus_alpha_bar_t = (1 - alpha_bar[t]) ** 0.5\n",
    "        sqrt_one_minus_alpha_bar_t = sqrt_one_minus_alpha_bar_t.view(-1, 1, 1, 1)\n",
    "    \n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x)\n",
    "    \n",
    "        return sqrt_alpha_bar_t * x + sqrt_one_minus_alpha_bar_t * noise, noise\n",
    "    \n",
    "    def get_loss_weight(self, t):\n",
    "        \"\"\"\n",
    "        Returns SNR-based weights with proper device handling.\n",
    "        Ensures SNR tensor is on same device as timestep tensor t.\n",
    "        \"\"\"\n",
    "        device = t.device\n",
    "        snr = self._move_to_device(self.snr, device)\n",
    "        return torch.clamp(snr[t], min=1.0, max=10.0)\n",
    "    \n",
    "    def get_beta(self, t):\n",
    "        \"\"\"\n",
    "        Device-safe beta accessor.\n",
    "        \"\"\"\n",
    "        device = t.device\n",
    "        return self._move_to_device(self.beta, device)[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f739c90f-95d5-4a21-b9a1-60c3c66551f2",
    "_uuid": "be5c9846-b68e-4753-9a55-65d710f84897",
    "collapsed": false,
    "id": "cR-HOWv4MFMR",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "UNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "0ec9648a-5eda-4d6e-ba25-5b5274b85952",
    "_uuid": "12b2be46-4a3f-4efd-a998-a9007c2a0898",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-16T09:43:30.120803Z",
     "iopub.status.busy": "2025-04-16T09:43:30.120505Z",
     "iopub.status.idle": "2025-04-16T09:43:36.224051Z",
     "shell.execute_reply": "2025-04-16T09:43:36.223150Z",
     "shell.execute_reply.started": "2025-04-16T09:43:30.120780Z"
    },
    "id": "QT2CixYnMGoR",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, time_emb_dim=None):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_channels) if time_emb_dim else None\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(8, out_channels)\n",
    "\n",
    "    def forward(self, x, time_emb=None):\n",
    "        h = self.conv1(x)\n",
    "        h = self.norm1(h)\n",
    "        h = F.relu(h)\n",
    "\n",
    "        # Time embedding injection\n",
    "        if self.time_mlp and time_emb is not None:\n",
    "            time_emb = self.time_mlp(time_emb)\n",
    "            time_emb = time_emb.reshape(time_emb.shape[0], -1, 1, 1)\n",
    "            h = h + time_emb\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.norm2(h)\n",
    "        h = F.relu(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "class UNet2D(nn.Module):\n",
    "    def __init__(self, in_channels=1, cond_channels=1, out_channels=1, features=[64, 128, 256], time_emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.time_dim = time_emb_dim\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "        )\n",
    "\n",
    "        # Learnable projection of spectrogram condition\n",
    "        self.cond_proj = nn.Conv2d(cond_channels, in_channels, kernel_size=1)\n",
    "\n",
    "        # Encoder (Downsampling)\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        prev_channels = in_channels\n",
    "        for feature in features:\n",
    "            self.encoder.append(ConvBlock(prev_channels, feature, time_emb_dim))\n",
    "            prev_channels = feature\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = ConvBlock(features[-1], features[-1] * 2, time_emb_dim)\n",
    "\n",
    "        # Decoder (Upsampling)\n",
    "        self.decoder = nn.ModuleList()\n",
    "        self.upsamples = nn.ModuleList()\n",
    "\n",
    "        reversed_features = list(reversed(features))\n",
    "        prev_channels = features[-1] * 2\n",
    "\n",
    "        for i, feature in enumerate(reversed_features):\n",
    "            self.upsamples.append(nn.ConvTranspose2d(prev_channels, feature, kernel_size=2, stride=2))\n",
    "            self.decoder.append(ConvBlock(feature * 2, feature, time_emb_dim))\n",
    "            prev_channels = feature\n",
    "\n",
    "        # Final output layer\n",
    "        self.final = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, cond_spec, timestep):\n",
    "        # x: [batch,1,time] → add pseudo-height for 2D conv\n",
    "        x = x.unsqueeze(2)           # → [batch,1,1,time]\n",
    "        # Project and fuse condition\n",
    "        cond = self.cond_proj(cond_spec)\n",
    "        x = x + cond\n",
    "\n",
    "        # Compute time embedding\n",
    "        t = self.time_mlp(timestep)\n",
    "\n",
    "        # Encoder with skip connections\n",
    "        skips = []\n",
    "        for encoder in self.encoder:\n",
    "            x = encoder(x, t)\n",
    "            skips.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x, t)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        skips = skips[::-1]  # Reverse for easier access\n",
    "\n",
    "        for i, (upsample, decoder) in enumerate(zip(self.upsamples, self.decoder)):\n",
    "            x = upsample(x)\n",
    "            skip = skips[i]\n",
    "\n",
    "            # Ensure matching shapes for concatenation\n",
    "            if x.shape != skip.shape:\n",
    "                x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "            # Concatenate with skip connection\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "\n",
    "            # Apply decoder block with time embedding\n",
    "            x = decoder(x, t)\n",
    "\n",
    "        out = self.final(x)\n",
    "        return out.squeeze(2)       # → [batch,out_channels,time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2195ca4b-d689-4a60-a3df-44f8ffd1de7f",
    "_uuid": "0a039c73-a3a8-4627-9eee-32edc1b6caaa",
    "collapsed": false,
    "id": "GJZnF6ij0Ild",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "1d98b16e-051e-483d-870e-47eda8c5c20d",
    "_uuid": "e4938b98-c065-4752-8c3c-e3fe17721927",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-16T09:44:59.665865Z",
     "iopub.status.busy": "2025-04-16T09:44:59.665579Z",
     "iopub.status.idle": "2025-04-16T09:44:59.677407Z",
     "shell.execute_reply": "2025-04-16T09:44:59.676602Z",
     "shell.execute_reply.started": "2025-04-16T09:44:59.665844Z"
    },
    "id": "60Z-1mM50DOh",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, clean_dir, noisy_dir, sample_rate=16000, n_mels=128, n_fft=1024, hop_length=256,\n",
    "                 chunk_width=None, cache_data=False):\n",
    "        self.clean_dir = clean_dir\n",
    "        self.noisy_dir = noisy_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.file_list = [f for f in os.listdir(clean_dir) if f.endswith(\".wav\")]\n",
    "        self.chunk_width = chunk_width  # Width in frames for random slicing\n",
    "        self.cache_data = cache_data\n",
    "        self.cached_data = {}\n",
    "\n",
    "        # use magnitude (power=1.0) instead of power spectrogram\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels,\n",
    "            power=1.0,  # Use magnitude spectrogram\n",
    "            normalized=True,  # Normalize the output\n",
    "        )\n",
    "\n",
    "        if cache_data:\n",
    "            print(\"Caching dataset...\")\n",
    "            for idx, file_name in enumerate(self.file_list):\n",
    "                if idx % 100 == 0:\n",
    "                    print(f\"Caching: {idx}/{len(self.file_list)}\")\n",
    "                clean_path = os.path.join(self.clean_dir, file_name)\n",
    "                noisy_path = os.path.join(self.noisy_dir, file_name)\n",
    "\n",
    "                clean = self._process_wav(clean_path)\n",
    "                noisy = self._process_wav(noisy_path)\n",
    "\n",
    "                if self.chunk_width:\n",
    "                    clean, noisy = self._random_chunk_pair(clean, noisy)\n",
    "\n",
    "                self.cached_data[file_name] = (clean, noisy)\n",
    "            print(\"Dataset cached successfully!\")\n",
    "\n",
    "    # def _process_wav(self, path):\n",
    "    #     waveform, sr = torchaudio.load(path)\n",
    "    #     if sr != self.sample_rate:\n",
    "    #         resampler = torchaudio.transforms.Resample(\n",
    "    #             orig_freq=sr, new_freq=self.sample_rate\n",
    "    #         )\n",
    "    #         waveform = resampler(waveform)\n",
    "    #     # Mel spectrogram to dB-scaled [0,1]\n",
    "    #     mel = self.mel_transform(waveform)\n",
    "    #     with torch.no_grad():\n",
    "    #         spec = 20 * torch.log10(torch.clamp(mel, min=1e-5)) - 20\n",
    "    #         spec = torch.clamp((spec + 100) / 100, 0.0, 1.0)\n",
    "    #     return spec\n",
    "\n",
    "    # def _random_chunk_pair(self, clean, noisy):\n",
    "    #     _, _, time_steps = clean.shape\n",
    "    #     if time_steps <= self.chunk_width:\n",
    "    #         # print(\"Oops\")\n",
    "    #         # Pad or truncate to match chunk_width\n",
    "    #         clean = F.pad(clean, (0, self.chunk_width - time_steps)) if time_steps < self.chunk_width else clean[:, :, :self.chunk_width]\n",
    "    #         noisy = F.pad(noisy, (0, self.chunk_width - time_steps)) if time_steps < self.chunk_width else noisy[:, :, :self.chunk_width]\n",
    "    #         return clean, noisy\n",
    "    #     # Random slicing for larger inputs\n",
    "    #     start = torch.randint(0, time_steps - self.chunk_width, (1,)).item()\n",
    "    #     end = start + self.chunk_width\n",
    "    #     return clean[:, :, start:end], noisy[:, :, start:end]\n",
    "\n",
    "\n",
    "    # def _normalize(self, tensor, eps=1e-5):\n",
    "    #     mean = tensor.mean()\n",
    "    #     std = tensor.std()\n",
    "    #     return (tensor - mean) / (std + eps)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_list[idx]\n",
    "        clean_waveform, _ = torchaudio.load(os.path.join(self.clean_dir, file_name))\n",
    "        noisy_waveform, _ = torchaudio.load(os.path.join(self.noisy_dir, file_name))\n",
    "        # compute noisy spectrogram with manual dB scaling\n",
    "        with torch.no_grad():\n",
    "            mel = self.mel_transform(noisy_waveform)\n",
    "            noisy_spec = 20 * torch.log10(torch.clamp(mel, min=1e-5)) - 20\n",
    "            noisy_spec = torch.clamp((noisy_spec + 100) / 100, 0.0, 1.0)\n",
    "        # noisy_spec = self._normalize(noisy_spec)\n",
    "        return clean_waveform, noisy_waveform, noisy_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "0bc55edd-9947-4064-95be-cc8503354554",
    "_uuid": "5ce2e584-551f-4822-91f6-93c6751f9cf4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-11T18:42:17.388295Z",
     "iopub.status.busy": "2025-04-11T18:42:17.387997Z",
     "iopub.status.idle": "2025-04-11T18:42:17.910350Z",
     "shell.execute_reply": "2025-04-11T18:42:17.909410Z",
     "shell.execute_reply.started": "2025-04-11T18:42:17.388273Z"
    },
    "id": "NHA5bWYD-EFi",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(clean_dir, noisy_dir, val_ratio=0.3, cache_data=False, chunk_width=None):\n",
    "    file_list = [f for f in os.listdir(clean_dir) if f.endswith(\".wav\")]\n",
    "    train_files, val_files = train_test_split(file_list, test_size=val_ratio, random_state=42)\n",
    "\n",
    "    train_dataset = SpectrogramDataset(\n",
    "        clean_dir, noisy_dir, cache_data=cache_data, chunk_width=chunk_width\n",
    "    )\n",
    "    val_dataset = SpectrogramDataset(\n",
    "        clean_dir, noisy_dir, cache_data=cache_data, chunk_width=chunk_width\n",
    "    )\n",
    "\n",
    "    train_dataset.file_list = train_files\n",
    "    val_dataset.file_list = val_files\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def create_dataloaders(clean_dir, noisy_dir, batch_size=8, num_workers=4, cache_data=False, chunk_width=None):\n",
    "    train_dataset, val_dataset = split_dataset(\n",
    "        clean_dir, noisy_dir, cache_data=cache_data, chunk_width=chunk_width\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "db1c3ff8-b720-41cb-8803-0f599cce50b4",
    "_uuid": "3afaca83-24c0-45b8-a797-60874c880f4c",
    "collapsed": false,
    "id": "v34kDrW0MUUP",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "8bdcbde0-5ce3-4573-a004-53f518e87472",
    "_uuid": "1117c881-860c-4dc7-946d-7166c18e9b5d",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-11T18:42:19.772933Z",
     "iopub.status.busy": "2025-04-11T18:42:19.772496Z",
     "iopub.status.idle": "2025-04-11T18:42:19.786796Z",
     "shell.execute_reply": "2025-04-11T18:42:19.785820Z",
     "shell.execute_reply.started": "2025-04-11T18:42:19.772905Z"
    },
    "id": "4w6n7mfpMcSp",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def train(model, train_loader, val_loader, scheduler, optimizer, epochs=10, device=\"cuda\",\n",
    "          checkpoint_interval=2, checkpoint_dir=\"checkpoints\", resume_from_checkpoint=None,\n",
    "          use_weighted_loss=True, clip_grad_norm=1.0, mixed_precision=True):\n",
    "\n",
    "    model = model.to(device)\n",
    "    scaler = torch.amp.GradScaler(enabled=mixed_precision)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    epoch_losses = []\n",
    "    start_epoch = 0\n",
    "\n",
    "    # Resume from checkpoint if specified\n",
    "    if resume_from_checkpoint:\n",
    "        checkpoint = torch.load(resume_from_checkpoint, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scaler.load_state_dict(checkpoint['scaler_state'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Resuming training from epoch {start_epoch}...\")\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} (Training)\", leave=False)\n",
    "\n",
    "        for clean, noisy_waveform, cond_spec in loop:\n",
    "            # move tensors to device\n",
    "            clean = clean.to(device)    # clean waveform]\n",
    "            noisy_waveform = noisy_waveform.to(device)  # noisy waveform\n",
    "            cond_spec = cond_spec.to(device)    # noisy spectrogram\n",
    "            batch_size = clean.size(0)\n",
    "\n",
    "            # sample timesteps\n",
    "            t = torch.randint(0, scheduler.timesteps, (batch_size,), device=device) # [batch_size,]\n",
    "\n",
    "            # noise scheduling (unchanged)\n",
    "            noise_scale = scheduler.alpha_bar[t].view(batch_size,1,1,1)\n",
    "            noise_scale_sqrt = noise_scale.sqrt()\n",
    "            m = (((1 - noise_scale) / noise_scale.sqrt()) ** 0.5).view(batch_size,1,1,1)\n",
    "            noise = torch.randn_like(clean)\n",
    "\n",
    "            noisy_t, combine_noise = (\n",
    "                (1-m) * noise_scale_sqrt * clean + m * noise_scale_sqrt * noisy_waveform + (1.0 - (1+m**2)*noise_scale).sqrt()*noise,\n",
    "                (m * noise_scale_sqrt * (noisy_waveform - clean) + (1.0 - (1+m**2)*noise_scale).sqrt()*noise) / (1.0 - noise_scale).sqrt()\n",
    "            )\n",
    "\n",
    "            print(f\"noisy_t: {noisy_t.shape}, combine_noise: {combine_noise.shape}, cond_spec: {cond_spec.shape}\")\n",
    "            # forward with spectrogram condition\n",
    "            predicted = model(noisy_t, cond_spec, t)\n",
    "            print(f\"predicted: {predicted.shape}\")\n",
    "\n",
    "            loss = F.mse_loss(combine_noise, predicted.squeeze(1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if clip_grad_norm > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} (Validation)\", leave=False):\n",
    "                clean = batch[0].to(device) if isinstance(batch, (list, tuple)) else batch.to(device)\n",
    "                batch_size = clean.size(0)\n",
    "                indices = torch.randperm(scheduler.timesteps, device=device)\n",
    "                t = indices[:batch_size] if batch_size <= scheduler.timesteps else indices.repeat(batch_size // len(indices) + 1)[:batch_size]\n",
    "\n",
    "                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                    noisy_t, _ = scheduler.add_noise(clean, t)\n",
    "                    predicted_x0 = model(noisy_t, t)\n",
    "                    val_loss += F.mse_loss(predicted_x0, clean).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        epoch_losses.append({'epoch': epoch+1, 'train_loss': avg_train_loss, 'val_loss': avg_val_loss})\n",
    "\n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f\"UNet_21_04_{epoch+1}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'scaler_state': scaler.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "\n",
    "    return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "9abbd164-25af-4f48-a9c5-2f1da20e566f",
    "_uuid": "35813451-0159-48e2-8b98-b80e23a99c3a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-11T18:42:26.188699Z",
     "iopub.status.busy": "2025-04-11T18:42:26.188416Z",
     "iopub.status.idle": "2025-04-11T19:37:08.526238Z",
     "shell.execute_reply": "2025-04-11T19:37:08.525275Z",
     "shell.execute_reply.started": "2025-04-11T18:42:26.188677Z"
    },
    "executionInfo": {
     "elapsed": 433435,
     "status": "error",
     "timestamp": 1743799061270,
     "user": {
      "displayName": "Pinki verma",
      "userId": "12203574888725304449"
     },
     "user_tz": -330
    },
    "id": "a0mWchNwWphv",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "7aa10f77-efc8-40c4-e21b-91a3d5593366",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "                                                              \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [1, 131074] at entry 0 and [1, 161620] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f5e7f2cef9b8>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m losses = train(\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-f18108768cfc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, scheduler, optimizer, epochs, device, checkpoint_interval, checkpoint_dir, resume_from_checkpoint, use_weighted_loss, clip_grad_norm, mixed_precision)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs} (Training)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoisy_waveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_spec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m# move tensors to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mclean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# clean waveform]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [1, 131074] at entry 0 and [1, 161620] at entry 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, optimizer, and scheduler\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = UNet2D(in_channels=1, out_channels=1, features=[64, 128, 256], time_emb_dim=256)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "scheduler = NoiseScheduler()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# Recommended chunk width (128 frames ≈ 2 seconds of audio)\n",
    "chunk_width = 128\n",
    "num_workers = min(8, os.cpu_count())\n",
    "\n",
    "# Create train and validation dataloaders with chunking\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    clean_dir=clean_train_path,\n",
    "    noisy_dir=noisy_train_path,\n",
    "    batch_size=64,\n",
    "    num_workers=num_workers,\n",
    "    cache_data=False,\n",
    "    chunk_width=chunk_width\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "losses = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    scheduler=scheduler,\n",
    "    optimizer=optimizer,\n",
    "    epochs=50,\n",
    "    device=device,\n",
    "    checkpoint_interval=2,\n",
    "    checkpoint_dir=\"./checkpoints\",\n",
    "    use_weighted_loss=True,\n",
    "    clip_grad_norm=1.0, \n",
    "    # resume_from_checkpoint=\"./checkpoints/UNet_New_11_04_30.pth\"\n",
    ")\n",
    "\n",
    "# Saving Epoch_losses for future use and concatenation purpose\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save losses with a timestamped filename\n",
    "with open(f\"losses_{timestamp}.pkl\", 'wb') as f:\n",
    "    pickle.dump(losses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract training and validation losses\n",
    "epochs = [entry['epoch'] for entry in losses]\n",
    "train_losses = [entry['train_loss'] for entry in losses]\n",
    "val_losses = [entry['val_loss'] for entry in losses]\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_losses, label='Training Loss', marker='o')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss', marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "F1_8MVRjRnnU"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7050064,
     "sourceId": 11277199,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
